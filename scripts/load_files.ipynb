{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it a banger? - Make your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some brief instructions on how to make the dataset I used in the article [Is it a banger?](https://nbviewer.jupyter.org/github/ddervs/is_it_a_banger/blob/master/scripts/is_it_a_banger.ipynb).\n",
    "\n",
    "I'm also going to assume you have downloaded the files in the [GitHub repository](https://github.com/ddervs/is_it_a_banger)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder structure\n",
    "\n",
    "We want to create a directory called `data`, with a subdirectory for each label, e.g.\n",
    "\n",
    "```\n",
    "data\n",
    "├── label_1\n",
    "├── label_2\n",
    "├──    ·\n",
    "├──    ·\n",
    "├──    ·\n",
    "└── label_k\n",
    "```\n",
    "\n",
    "In each label subdirectory, we have a text-file, where each line is the URL of a YouTube track or playlist with the relevant audio data. \n",
    "\n",
    "For the article, we simply have\n",
    "\n",
    "```\n",
    "data\n",
    "├── banger\n",
    "│   └── URL_banger.txt\n",
    "└── not_a_banger\n",
    "    └── URL_not_a_banger.txt\n",
    "```\n",
    "\n",
    "You can see the URLs used in the article at [URL_banger.txt](https://github.com/ddervs/is_it_a_banger/blob/master/data/banger/URL_banger.txt) and [URL_not_a_banger.txt](https://github.com/ddervs/is_it_a_banger/blob/master/data/not_a_banger/URL_not_a_banger.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to run the following command in the directory `is_it_a_banger/scripts/`\n",
    "\n",
    "```\n",
    "./scripts/prepare_data_files.sh data 5\n",
    "```\n",
    "\n",
    "where `5` is the audio segment length in seconds. Note that this script requires `ffmpeg` and `youtube-dl` to work.\n",
    "\n",
    "After running the script, you should have in each label subdirectory a bunch of 5 second `.wav` audio files.\n",
    "\n",
    "We then need the following python to generate the pandas DataFrame from the generated audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get filenames and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parent_dir = '../data'\n",
    "parent_dir_contents = [os.path.join(parent_dir, dirname) for dirname in os.listdir(parent_dir)]\n",
    "sub_dirs = [filename if os.path.isdir(filename) else None for filename in parent_dir_contents]\n",
    "sub_dirs = list(filter(None.__ne__, sub_dirs))\n",
    "labels_list = [os.path.relpath(path, parent_dir) for path in sub_dirs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n",
    "\n",
    "We're going to use the `librosa` library for processing the audio signal. We'll keep the raw audio samples and compute a log spectrogram.\n",
    "\n",
    "Note that we clip samples at the end of the audio file, as the combination of running `ffmpeg` earlier and resampling to 22.05kHz means the audio sample arrays don't have uniform length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_features(file_name, sample_rate=22050, segment_time=5, samples_to_clip=500):\n",
    "    audio, sample_rate = librosa.load(file_name, sr=sample_rate)\n",
    "    end_idx = (sample_rate * segment_time) - samples_to_clip # remove some end samples as not strictly uniform size\n",
    "    audio = audio[0:end_idx]\n",
    "    log_specgram = librosa.logamplitude(np.abs(librosa.stft(audio))**2, ref_power=np.max)\n",
    "    features = {\"audio\": audio, \"log_specgram\": log_specgram}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn labels into 'one-hot' vector encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(label, labels_list):\n",
    "    n_labels = len(labels_list)\n",
    "    one_hot_encoded = np.zeros(n_labels)\n",
    "    for idx, cmp in enumerate(labels_list):\n",
    "        if label == cmp:\n",
    "            one_hot_encoded[idx] = 1                     \n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim file list\n",
    "\n",
    "Only include a fraction of audio files for a given track to avoid training set 1) having too many highly correlated data points, and 2) having too large a file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim_file_list(fnames_list, p_include=1.0):\n",
    "    fnames_list = np.asarray(fnames_list)\n",
    "    include = np.random.rand(*fnames_list.shape)\n",
    "    fnames_list = fnames_list[include < p_include]\n",
    "    return fnames_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataFrame from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_audio_files(parent_dir, sub_dirs_list, labels_list, file_ext='*.wav', p_include=1.0,\\\n",
    "                      sample_rate=22050, segment_time=5, samples_to_clip=500):\n",
    "    data = []\n",
    "    index = []\n",
    "    for label_idx, sub_dir in enumerate(sub_dirs_list):\n",
    "        fnames_list = glob.glob(os.path.join(sub_dir, file_ext))\n",
    "        fnames_list = trim_file_list(fnames_list, p_include=p_include)\n",
    "        for fname in fnames_list:\n",
    "            print(\"Processing \" + os.path.basename(fname))\n",
    "            features = extract_features(fname, segment_time=segment_time, \\\n",
    "                                        sample_rate=sample_rate, samples_to_clip=samples_to_clip)\n",
    "            label = labels_list[label_idx]\n",
    "            label_one_hot = one_hot_encode(label, labels_list)\n",
    "            features['label'] = label\n",
    "            features[\"label_one_hot\"] = label_one_hot\n",
    "            data.append(features)\n",
    "            index.append(os.path.basename(fname))\n",
    "    return pd.DataFrame(data, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_audio_files(parent_dir, sub_dirs, labels_list, p_include=0.1, segment_time=5, samples_to_clip=1100)\n",
    "df = df.iloc[np.random.permutation(len(df))] # shuffle rows\n",
    "df.to_pickle(os.path.join(parent_dir, 'processed_dataset.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
